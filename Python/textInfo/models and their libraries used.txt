The approximate uncompressed image sizes (in kilobytes) for the given resolutions, assuming RGB color (3 channels), are:
640x480: 900 KB
1080x720: 2278.13 KB (~2.2 MB)
1920x1080: 6075 KB (~6 MB)
These values are for uncompressed images. Actual file sizes will vary depending on compression settings (e.g., JPEG, PNG)

JPEG compression significantly reduces file size by discarding some image data based on quality settings (e.g., 
cv.IMWRITE_JPEG_QUALITY in OpenCV). The actual size depends on the compression ratio, image complexity, and quality level.

Here‚Äôs an estimate for JPEG sizes at typical compression levels:
JPEG Compression Quality vs. Size Estimates:
Quality = 90 (High Quality): ~15%-20% of the uncompressed size.
Quality = 75 (Medium Quality): ~10%-15% of the uncompressed size.
Quality = 50 (Low Quality): ~5%-10% of the uncompressed size.

Here are the approximate JPEG file sizes (in kilobytes) for different quality levels:
High Quality (90%):
640x480: 180 KB
1080x720: 455.63 KB (~0.45 MB)
1920x1080: 1215 KB (~1.2 MB)

Medium Quality (75%):
640x480: 135 KB
1080x720: 341.72 KB (~0.34 MB)
1920x1080: 911.25 KB (~0.91 MB)

Low Quality (50%):
640x480: 90 KB
1080x720: 227.81 KB (~0.22 MB)
1920x1080: 607.5 KB (~0.6 MB)

-------------------------------------------------------------------------------------------------------------------------------------

1) OpenCV:- used to capture the frame and perform functions on that frame. Detect camera and capture the frame, add texts on the 
frame, display the frame and with the ENTER and MOUSE CLICK take that particular frame for the process

2) mediapipe:- 
used for:-
A) Face detection and landmark detection
input:-Still images, decoded video frames, live video feed
The Face Detector outputs the following results:
Bounding boxes for detected faces in an image frame.
Coordinates for 6 face landmarks for each detected face.

configuration options:-
1)running_mode	
Sets the running mode for the task. There are three modes:
IMAGE: The mode for single image inputs.
VIDEO: The mode for decoded frames of a video.
LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, resultListener must be called to set up a 
listener to receive results asynchronously.
Default:- IMAGE

2)min_detection_confidence
The minimum confidence score for the face detection to be considered successful.
Default:- 0.5

3)min_suppression_threshold	
The minimum non-maximum-suppression threshold for face detection to be considered overlapped.
Default:-0.3

result_callback
Sets the result listener to receive the detection results asynchronously when the Face Detector is in the live stream mode. Can only 
be used when running mode is set to LIVE_STREAM.
Default:- Not set

Models
Face detection models can vary depending on their intended use cases, such as short-range and long-range detection. Models also 
typically make trade-offs between performance, accuracy, resolution, and resource requirements, and in some cases, include additional 
features.
The models listed in this section are variants of BlazeFace, a lightweight and accurate face detector optimized for mobile GPU 
inference. BlazeFace models are suitable for applications like 3D facial keypoint estimation, expression classification, and face 
region segmentation. BlazeFace uses a lightweight feature extraction network similar to MobileNetV1/V2.

BlazeFace (short-range)
A lightweight model for detecting single or multiple faces within selfie-like images from a smartphone camera or webcam. The model is 
optimized for front-facing phone camera images at short range. The model architecture uses a Single Shot Detector (SSD) convolutional 
network technique with a custom encoder. For more information, see the research paper on Single Shot MultiBox Detector.			
Input shape = 128 x 128 
Quantization type = float 16	
Model Card = info	
Versions = Latest

BlazeFace (full-range)
A relatively lightweight model for detecting single or multiple faces within images from a smartphone camera or webcam. The model is 
optimized for full-range images, like those taken with a back-facing phone camera images. The model architecture uses a technique 
similar to a CenterNet convolutional network with a custom encoder.
Input shape = 128 x 128 
Quantization type = float 16	
Model Card = info	
Versions = Coming soon

BlazeFace Sparse (full-range)
A lighter version of the regular full-range BlazeFace model, roughly 60% smaller in size. The model is optimized for full-range 
images, like those taken with a back-facing phone camera images. The model architecture uses a technique similar to a CenterNet 
convolutional network with a custom encoder.
Input shape = 128 x 128 
Quantization type = float 16	
Model Card = info	
Versions = Coming soon

Task benchmarks
Here's the task benchmarks for the whole pipeline based on the above pre-trained models. The latency result is the average latency on 
Pixel 6 using CPU / GPU.
Model Name = BlazeFace (short-range)	
CPU Latency = 2.94ms	
GPU Latency = 7.41ms

WEBSITE:- https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector

MediaPipe Face Detection is an ultrafast face detection solution that comes with 6 landmarks and multi-face support. It is based on 
BlazeFace, a lightweight and well-performing face detector tailored for mobile GPU inference. The detector‚Äôs super-realtime 
performance enables it to be applied to any live viewfinder experience that requires an accurate facial region of interest as an 
input for other task-specific models, such as 3D facial keypoint estimation (e.g., MediaPipe Face Mesh), facial features or 
expression classification, and face region segmentation. BlazeFace uses a lightweight feature extraction network inspired by, but 
distinct from MobileNetV1/V2, a GPU-friendly anchor scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie 
resolution strategy alternative to non-maximum suppression. For more information about BlazeFace, please see the Resources section.

Configuration Options
Naming style and availability may differ slightly across platforms/languages.

1)model_selection
An integer index 0 or 1. Use 0 to select a short-range model that works best for faces within 2 meters from the camera, and 1 for a 
full-range model best for faces within 5 meters. For the full-range option, a sparse model is used for its improved inference speed. 
Please refer to the model cards for details. Default to 0 if not specified.

Note: Not available for JavaScript (use ‚Äúmodel‚Äù instead).

2)model
A string value to indicate which model should be used. Use ‚Äúshort‚Äù to select a short-range model that works best for faces within 2 
meters from the camera, and ‚Äúfull‚Äù for a full-range model best for faces within 5 meters. For the full-range option, a sparse model 
is used for its improved inference speed. Please refer to the model cards for details. Default to empty string.

Note: Valid only for JavaScript solution.

3)selfie_mode
A boolean value to indicate whether to flip the images/video frames horizontally or not. Default to false.

Note: Valid only for JavaScript solution.

4)min_detection_confidence
Minimum confidence value ([0.0, 1.0]) from the face detection model for the detection to be considered successful. Default to 0.5.

Output
Naming style may differ slightly across platforms/languages.

5)detections
Collection of detected faces, where each face is represented as a detection proto message that contains a bounding box and 6 key 
points (right eye, left eye, nose tip, mouth center, right ear tragion, and left ear tragion). The bounding box is composed of xmin 
and width (both normalized to [0.0, 1.0] by the image width) and ymin and height (both normalized to [0.0, 1.0] by the image height). Each key point is composed of x and y, which are normalized to [0.0, 1.0] by the image width and height respectively.

Python Solution API
Please first follow general instructions to install MediaPipe Python package, then learn more in the companion Python Colab and the usage example below.

Supported configuration options:

model_selection
min_detection_confidence
import cv2
import mediapipe as mp
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

# For static images:
IMAGE_FILES = []
with mp_face_detection.FaceDetection(
    model_selection=1, min_detection_confidence=0.5) as face_detection:
  for idx, file in enumerate(IMAGE_FILES):
    image = cv2.imread(file)
    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.
    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw face detections of each face.
    if not results.detections:
      continue
    annotated_image = image.copy()
    for detection in results.detections:
      print('Nose tip:')
      print(mp_face_detection.get_key_point(
          detection, mp_face_detection.FaceKeyPoint.NOSE_TIP))
      mp_drawing.draw_detection(annotated_image, detection)
    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)

# For webcam input:
cap = cv2.VideoCapture(0)
with mp_face_detection.FaceDetection(
    model_selection=0, min_detection_confidence=0.5) as face_detection:
  while cap.isOpened():
    success, image = cap.read()
    if not success:
      print("Ignoring empty camera frame.")
      # If loading a video, use 'break' instead of 'continue'.
      continue

    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    image.flags.writeable = False
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_detection.process(image)

    # Draw the face detection annotations on the image.
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    if results.detections:
      for detection in results.detections:
        mp_drawing.draw_detection(image, detection)
    # Flip the image horizontally for a selfie-view display.
    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))
    if cv2.waitKey(5) & 0xFF == 27:
      break
cap.release()

JavaScript Solution API
Please first see general introduction on MediaPipe in JavaScript, then learn more in the companion web demo and the following usage 
example.

Supported face detection options:

selfieMode
model
minDetectionConfidence
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/face_detection.js" crossorigin="anonymous"></script>
</head>

<body>
  <div class="container">
    <video class="input_video"></video>
    <canvas class="output_canvas" width="1280px" height="720px"></canvas>
  </div>
</body>
</html>
<script type="module">
const videoElement = document.getElementsByClassName('input_video')[0];
const canvasElement = document.getElementsByClassName('output_canvas')[0];
const canvasCtx = canvasElement.getContext('2d');
const drawingUtils = window;

function onResults(results) {
  // Draw the overlays.
  canvasCtx.save();
  canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
  canvasCtx.drawImage(
      results.image, 0, 0, canvasElement.width, canvasElement.height);
  if (results.detections.length > 0) {
    drawingUtils.drawRectangle(
        canvasCtx, results.detections[0].boundingBox,
        {color: 'blue', lineWidth: 4, fillColor: '#00000000'});
    drawingUtils.drawLandmarks(canvasCtx, results.detections[0].landmarks, {
      color: 'red',
      radius: 5,
    });
  }
  canvasCtx.restore();
}

const faceDetection = new FaceDetection({locateFile: (file) => {
  return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.0/${file}`;
}});
faceDetection.setOptions({
  model: 'short',
  minDetectionConfidence: 0.5
});
faceDetection.onResults(onResults);

const camera = new Camera(videoElement, {
  onFrame: async () => {
    await faceDetection.send({image: videoElement});
  },
  width: 1280,
  height: 720
});
camera.start();
</script>

WEBSITE:- https://mediapipe.readthedocs.io/en/latest/solutions/face_detection.html

Mediapipe's BlazeFace face detection model, while a powerful tool, has some limitations: 
Uneven Lighting: As you mentioned, significant variations in lighting, such as very low light or strong backlighting, can 
significantly impact BlazeFace's accuracy. The model may struggle to detect faces in these conditions due to the difficulty in 
distinguishing facial features from shadows or highlights.

Occlusion: If a significant portion of the face is occluded (covered) by objects like hands, hair, or accessories, BlazeFace might 
fail to detect the face accurately or might not detect it at all.

Small Faces: The model may have difficulty detecting very small faces in the image, especially if they are far away or the image 
resolution is low. ¬† 

Pose Variation: While BlazeFace can handle some head rotation, extreme poses (looking away from the camera at significant angles) 
can reduce detection accuracy.

Facial Expressions: Highly expressive faces (e.g., wide smiles, strong frowns) can sometimes confuse the model, leading to 
inaccurate detections or missed detections.

Facial Hair: Dense facial hair (like long beards) can sometimes interfere with the model's ability to accurately locate facial 
landmarks.

It's important to note that:
These limitations are common to many face detection models, not just BlazeFace.
Researchers are constantly working on improving the robustness of face detection algorithms to overcome these challenges. ¬† 

WEBSITE:- 1)https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75#:~:text=Thanks%
20to%20libraries%20such%20as,on%20an%20average%20mobile%20device.
2)https://github.com/hollance/BlazeFace-PyTorch
3)https://pmc.ncbi.nlm.nih.gov/articles/PMC10390551/#:~:text=In%20order%20to%20address%20this%20problem%2C%20Lu%20et%20al.&proposed%
20a%20method%20based%20on,of%20the%20face%20recognition%20system.

B) Face landmarks detection
The MediaPipe Face Landmarker task lets you detect face landmarks and facial expressions in images and videos. You can use this 
task to identify human facial expressions, apply facial filters and effects, and create virtual avatars. This task uses machine 
learning (ML) models that can work with single images or a continuous stream of images. The task outputs 3-dimensional face 
landmarks, blendshape scores (coefficients representing facial expression) to infer detailed facial surfaces in real-time, and 
transformation matrices to perform the transformations required for effects rendering.

Features
Input image processing - Processing includes image rotation, resizing, normalization, and color space conversion.
Score threshold - Filter results based on prediction scores.

Task inputs	
The Face Landmarker accepts an input of one of the following data types:
Still images
Decoded video frames
Live video feed
Task outputs
The Face Landmarker outputs the following results:
Bounding boxes for detected faces in an image frame.
A complete face mesh for each detected face, with blendshape scores denoting facial expressions and coordinates for facial landmarks.

Configurations options
This task has the following configuration options:

1)running_mode	
Sets the running mode for the task. There are three modes:
IMAGE: The mode for single image inputs.
VIDEO: The mode for decoded frames of a video.
LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, resultListener must be called to set up a 
listener to receive results asynchronously.
Value range:-{IMAGE, VIDEO, LIVE_STREAM}	
Default:-IMAGE

2)num_faces	
The maximum number of faces that can be detected by the the FaceLandmarker. Smoothing is only applied when num_faces is set to 1.	
Value range:- Integer > 0	
Default:-1

3)min_face_detection_confidence	
The minimum confidence score for the face detection to be considered successful.	
Value range:- Float [0.0,1.0]	
Default:- 0.5

4)min_face_presence_confidence	
The minimum confidence score of face presence score in the face landmark detection.	
Value range:- Float [0.0,1.0]	
Default:- 0.5

5)min_tracking_confidence	
The minimum confidence score for the face tracking to be considered successful.	
Value range:- Float [0.0,1.0]	
Default:- 0.5

6)output_face_blendshapes	
Whether Face Landmarker outputs face blendshapes. Face blendshapes are used for rendering the 3D face model.	
Value range:- Boolean	
Default:- False

7)output_facial_transformation_matrixes	Whether FaceLandmarker outputs the facial transformation matrix. FaceLandmarker uses the matrix to transform the face landmarks from a canonical face model to the detected face, so users can apply effects on the detected landmarks.	
Value range:- Boolean	
Default:- False

7)result_callback	Sets the result listener to receive the landmarker results asynchronously when FaceLandmarker is in the live stream mode. Can only be used when running mode is set to LIVE_STREAM	
Value range:- ResultListener	
Default:- N/A

Models
The Face Landmarker uses a series of models to predict face landmarks. The first model detects faces, a second model locates landmarks
 on the detected faces, and a third model uses those landmarks to identify facial features and expressions.

The following models are packaged together into a downloadable model bundle:

Face detection model: detects the presence of faces with a few key facial landmarks.
Face mesh model: adds a complete mapping of the face. The model outputs an estimate of 478 3-dimensional face landmarks.
Blendshape prediction model: receives output from the face mesh model predicts 52 blendshape scores, which are coefficients 
representing facial different expressions.
The face detection model is the BlazeFace short-range model, a lightweight and accurate face detector optimized for mobile GPU 
inference. For more information, see the Face Detector task.

Model bundle				
FaceLandmarker	

Input shape:- 
FaceDetector: 192 x 192
FaceMesh-V2: 256 x 256
Blendshape: 1 x 146 x 2	
Data type:- float 16	

Model Cards:- 
FaceDetector
FaceMesh-V2
Blendshape

Versions
Latest

FaceDetector:- https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf
FaceMesh-V2:- https://storage.googleapis.com/mediapipe-assets/Model%20Card%20MediaPipe%20Face%20Mesh%20V2.pdf
Blendshape:- https://storage.googleapis.com/mediapipe-assets/Model%20Card%20Blendshape%20V2.pdf

WEBSITE:- https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker

C) Object detection:
The MediaPipe Object Detector task lets you detect the presence and location of multiple classes of objects within images or videos. 
For example, an object detector can locate dogs in an image. This task operates on image data with a machine learning (ML) model, 
accepting static data or a continuous video stream as input and outputting a list of detection results. Each detection result 
represents an object that appears within the image or video.

Features
Input image processing - Processing includes image rotation, resizing, normalization, and color space conversion.
Label map locale - Set the language used for display names
Score threshold - Filter results based on prediction scores.
Top-k detection - Filter the number detection results.
Label allowlist and denylist - Specify the categories detected.

Task inputs	
The Object Detector API accepts an input of one of the following data types:
Still images
Decoded video frames
Live video feed

Task outputs
The Object Detector API outputs the following results for detected objects:
Category of object
Probability score
Bounding box coordinates

running_mode	
Sets the running mode for the task. There are three modes:
IMAGE: The mode for single image inputs.
VIDEO: The mode for decoded frames of a video.
LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, resultListener must be called to set up a 
listener to receive results asynchronously.
Value range:- {IMAGE, VIDEO, LIVE_STREAM}	
Default:- IMAGE

display_names	
Sets the language of labels to use for display names provided in the metadata of the task's model, if available. Default is en for 
English. You can add localized labels to the metadata of a custom model using the TensorFlow Lite Metadata Writer API	
Value range:- Locale code	
Default:- en
max_results	
Sets the optional maximum number of top-scored detection results to return.	
Value range:- Any positive numbers	
Default:- -1 (all results are returned)
score_threshold	
Sets the prediction score threshold that overrides the one provided in the model metadata (if any). Results below this value are 
rejected.	
Value range:- Any float	
Default:- Not set
category_allowlist	
Sets the optional list of allowed category names. If non-empty, detection results whose category name is not in this set will be 
filtered out. Duplicate or unknown category names are ignored. This option is mutually exclusive with category_denylist and using both
results in an error.	
Value range:- Any strings	
Default:- Not set
category_denylist	
Sets the optional list of category names that are not allowed. If non-empty, detection results whose category name is in this set will
be filtered out. Duplicate or unknown category names are ignored. This option is mutually exclusive with category_allowlist and using both results in an error.	
Value range:- Any strings	
Default:- Not set

Models
The Object Detector API requires an object detection model to be downloaded and stored in your project directory. If you do not 
already have a model, start with the default, recommended model. The other models presented in this section make trade-offs between 
latency and accuracy.

EfficientDet-Lite0 model (Recommended)
The EfficientDet-Lite0 model uses an EfficientNet-Lite0 backbone with a 320x320 input size and BiFPN feature network. The model was 
trained with the COCO dataset, a large-scale object detection dataset that contains 1.5 million object instances and 80 object labels.
See the full list of supported labels. EfficientDet-Lite0 is available as an int8, float16, or float32. This model is recommended 
because it strikes a balance between latency and accuracy. It is both accurate and lightweight enough for many use cases.

Model name			Input shape	Quantization type	Versions
EfficientDet-Lite0 (int8)	320 x 320	int8			Latest
EfficientDet-Lite0 (float 16)	320 x 320	float 16		Latest
EfficientDet-Lite0 (float 32)	320 x 320	None (float32)		Latest

EfficientDet-Lite2 model
The EfficientDet-Lite2 model uses an EfficientNet-Lite2 backbone with a 448x448 input size and BiFPN feature network. The model was 
trained with the COCO dataset, a large-scale object detection dataset that contains 1.5 million object instances and 80 object labels.
See the full list of supported labels. EfficientDet-Lite2 is available as an int8, float16, or float32 model. This model is generally more accurate than EfficientDet-Lite0, but is also slower and more memory intensive. This model is appropriate for use cases where accuracy is a greater priority to speed and size.

Model name			Input shape	Quantization type	Versions
EfficientDet-Lite2 (int8)	448 x 448	int8			Latest
EfficientDet-Lite2 (float 16)	448 x 448	float 16		Latest
EfficientDet-Lite2 (float 32)	448 x 448	None (float32)		Latest

SSD MobileNetV2 model
The SSD MobileNetV2 model uses a MobileNetV2 backbone with a 256x256 input size and SSD feature network. The model was trained with 
the COCO dataset, a large-scale object detection dataset that contains 1.5 million object instances and 80 object labels. See the full
list of supported labels. SSD MobileNetV2 is available as an int8 and float 32 model. This model is faster and lighter than 
EfficientDet-Lite0, but is also generally less accurate. This model is appropriate for use cases that require a fast, lightweight 
model that sacrifices some accuracy.

Model name			Input shape	Quantization type	Versions
SSDMobileNet-V2 (int8)		256 x 256	int8			Latest
SSDMobileNet-V2 (float 32)	256 x 256	None (float32)		Latest

Model requirements and metadata
This section describes the requirements for custom models if you decide to build a model to use with this task. Custom models must be
in TensorFlow Lite format and must include metadata describing the operating parameters of the model.

Design requirements
Input			Shape						Description
Input image		Float32 tensor of shape[1, height, width, 3]	The normalized input image.

Output			Shape						Description
detection_boxes		Float32 tensor of shape [1, num_boxes, 4]	Box location of each detected object.
detection_classes	Float32 tensor of shape [1, num_boxes]		Indices of the class names for each detected object.
detection_scores	float32 tensor of shape [1, num_boxes]		Prediction scores for each detected object.
num_boxes		Float32 tensor of size 1			The number of detected boxes.

Metadata requirements
Parameter			            Description							                                              Description
input_norm_mean		    The mean value used in the input tensor normalization.			            The normalized input image.
input_norm_std		    The field norm used in the input tensor normalization.			            Box location of each detected object.
label_file_paths	    The paths to the category tensor label files.                           Indices of the class names for each 										If the model does not have any label files, pass an empty list.		detected object.                	
score_calibration_md	Information on the score calibration operation in the classification    Prediction scores for each detected object.
			                tensor. This parameter is not required if the model does not use 
			                score calibration.		
num_boxes		           Float32 tensor of size 1						                                     The number of detected boxes.

Task benchmarks
Here's the task benchmarks for the above pre-trained models. The latency result is the average latency on Pixel 6 using CPU / GPU.
Model Name				CPU Latency		GPU Latency
EfficientDet-Lite0 float32 model	61.30ms			27.83ms
EfficientDet-Lite0 float16 model	53.97ms			27.97ms 
EfficientDet-Lite0 int8 model		29.31ms			-
EfficientDet-Lite2 float32 model	197.98ms		41.15ms
EfficientDet-Lite2 float16 model	198.77ms		47.31ms
EfficientDet-Lite2 int8 model		70.91ms			-
SSD MobileNetV2 float32 model		36.30ms			24.01ms
SSD MobileNetV2 float16 model		37.35ms			28.16ms

Here is the performance of each EfficientDet-Lite models compared to each others.

Model architecture	Size(MB)*	Latency(ms)**	Average Precision***
EfficientDet-Lite0	4.4		      37		        25.69%
EfficientDet-Lite1	5.8		      49		        30.55%
EfficientDet-Lite2	7.2		      69		        33.97%
EfficientDet-Lite3	11.4		    116		        37.70%
EfficientDet-Lite4	19.9		    260		        41.96%

---------------------------------------------------------------------------------------------------------------------------------------

Minimum distance of face from the camera:
The formula used to calculate the distance from the camera to the face is based on the principle of triangulation in computer vision, 
where the physical size of an object (in this case, the face) is known, and the apparent size in the image (width of the bounding box)
 is used to estimate the distance.

The formula used here:

Distance=ùëì‚ãÖùêª/‚Ñé‚Äã
 
Where:
f: Focal length of the camera (in pixels).
H: Real-world height/width of the object (e.g., the face width in cm).
h: Height/width of the object in the image (in pixels).
In the provided code, the constant 5000 appears to represent a calibrated value that includes both ùëì and H. This implies that the 
distance is inversely proportional to the face's perceived width in pixels.

Proof of Correctness:
Calibration-based Approach: This method is widely used in object detection and computer vision. For example:

Citation 1: OpenCV's documentation on camera calibration and 3D reconstruction uses a similar principle to measure distances. (OpenCV Docs)
https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html

Citation 2: The formula is also discussed in tutorials on distance estimation in computer vision, such as the PyImageSearch article on measuring distances to objects with a camera. (PyImageSearch Article)
Validation through Experimentation:
https://pyimagesearch.com/2015/01/19/find-distance-camera-objectmarker-using-python-opencv/

You can validate this method by placing a known object (e.g., a face or a marker) at a measured distance from the camera. Check if the distance computed by the code matches the actual distance. Minor discrepancies might arise due to calibration errors.
